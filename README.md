ğŸ“„ MiniBot â€“ RAG PDF Chatbot

MiniBot is a Retrieval Augmented Generation (RAG) based chatbot that allows users to ask questions from a PDF document using a local LLM powered by Ollama.

This project demonstrates how modern AI applications combine:

ğŸ“š Document Retrieval (FAISS Vector Database)

ğŸ¤– Local LLM Inference (Ollama)

ğŸ” Semantic Search (Embeddings)

ğŸ’¬ Interactive UI (Gradio)


ğŸš€ Features

=> Chat with any PDF document

=> Fully offline AI chatbot

=> Uses local LLM (llama3.2:1b) â€“ low RAM friendly

=> Uses FAISS vector search for fast retrieval

=> Beginner friendly and lightweight

=> Privacy-safe (documents stay local)


ğŸ§  Tech Stack

Technology   -     	Purpose

Python	     -      Core programming language

LangChain	   -      RAG pipeline orchestration

Ollama	     -      Local LLM hosting

FAISS	       -      Vector similarity search

Gradio	     -      Chat UI interface

PyPDF	       -      PDF text extraction



ğŸ“‚ Project Structure


Minibot/
â”‚

â”œâ”€â”€ app.py              # Main chatbot application

â”œâ”€â”€ policy.pdf          # User adds their own PDF here

â”œâ”€â”€ .gitignore

â”œâ”€â”€ requirements.txt    # Dependency list

â””â”€â”€ README.md

âš™ï¸ Installation Guide

âœ… Step 1 â€” Clone Repository
    git clone https://github.com/sbbala07/Minibot-RAG-pdf.git
    
    cd Minibot-RAG-pdf

âœ… Step 2 â€” Install Python Dependencies
    
    python -m pip install -U langchain langchain-community langchain-ollama faiss-cpu gradio pypdf

âœ… Step 3 â€” Install Ollama
    Download from:
    ğŸ‘‰ https://ollama.com/download
    After installation, verify:
    
    ollama --version
    
âœ… Step 4 â€” Download Required Models
  
    ollama pull llama3.2:1b
    ollama pull nomic-embed-text

âœ… Step 5 â€” Add Your PDF
    Place your PDF inside project folder and rename it:
    
    policy.pdf    # Any pdf of your choice

âœ… Step 6 â€” Run Ollama Server
    
    ollama serve

âœ… Step 7 â€” Run Chatbot
    Open new terminal and run:
   
    python app.py

âœ… Step 8 â€” Open Browser
    You will see:
   
    http://127.0.0.1:7860
Open it and start chatting with your PDF.

ğŸ”§ How It Works (RAG Pipeline)


    PDF â†’  Text Split â†’  Embeddings â†’  FAISS Vector Store
                      â†“
    User Question â†’ Similarity Search â†’ Context Retrieval
                      â†“
          Local LLM (Ollama)
                      â†“
            Answer Generation

ğŸ“Œ Why RAG?
- RAG improves LLM accuracy by:
- Preventing hallucinations
- Using real document context
- Making AI responses reliable
- Keeping data private and local

âš¡ Performance Notes
- First query may take ~10 seconds (model warm-up)
- Later queries become faster
- Designed for low GPU / CPU machines

ğŸ”’ Privacy Advantage

This chatbot runs fully locally:

No cloud API usage

No data sharing

Safe for sensitive documents

ğŸ§ª Future Improvements

~ Multiple PDF support

~ Streaming responses

~ Better UI styling

~ Model fine-tuning

~ Chat memory improvement

~ Planned: Dockerization and UI improvements



ğŸ‘¨â€ğŸ’» Author

Balachandran

AI & Data Science learner with hands-on experience building local LLM applications using LangChain, Ollama, and FAISS.


â­ Support

If you like this project:

- Star â­ the repository
- Share feedback
- Suggest improvements

ğŸ“œ License

This project is open-source and available for educational purposes.

ğŸ¯ Learning Outcome

This project demonstrates:
- Building real-world AI applications
- Using local LLMs
- Implementing Retrieval Augmented Generation
- Integrating vector databases
- Creating interactive AI UI

This project helped me understand practical challenges such as model latency, vector search tuning, and Gradio message formats.

